{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LeNet 网络 - 卷积神经网络**\n",
    "\n",
    "\n",
    "**参考文献：** [1] LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.\n",
    "https://www.researchgate.net/publication/2985446_Gradient-Based_Learning_Applied_to_Document_Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"LeNet-5.png\" width = 800 height = 400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet 分为卷积层和全连接层两个部分。\n",
    "\n",
    "**卷积层块**里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间格式，如线条的物体局部，之后最大池化层则用来除外卷积层对位置的敏感性。卷积层块由两个 这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 $5\\times 5$ 的窗口，并在 输出上使用sigmoid激活函数。\n",
    "第一个卷积层输出通道为6，第2个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层窗口形状均为 $2\\times 2$, 且步幅为 2. 由于池化窗口与步幅形状相同，池化窗口在输出上每次滑动所覆盖的区域互不重叠。\n",
    "\n",
    "卷积层的输出形状为（批大小，通道，高，宽）。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten)。也就是说，全连接层的输入形状将变成 二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **通过Sequential类来实现LeNet模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd,gluon,init,nd\n",
    "from mxnet.gluon import loss as gloss,nn\n",
    "import time \n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels = 6, kernel_size = 5, activation = \"sigmoid\"),\n",
    "        nn.MaxPool2D(pool_size=2, strides = 2),\n",
    "        nn.Conv2D(channels = 16, kernel_size = 5, activation = \"sigmoid\"),\n",
    "        nn.MaxPool2D(pool_size = 2, strides = 2),\n",
    "        nn.Dense(120,activation = \"sigmoid\"), # Dense会默认将（批大小，通道，高，宽）转换为（批大小，通道*高*宽）\n",
    "        nn.Dense(84,activation = \"sigmoid\"),\n",
    "        nn.Dense(10)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 构造一个高和宽为32*32的单通道样本，并逐层进行前向计算来查看每个层的输出形状\n",
    "# X = nd.random.uniform(shape = (1,1,32,32))\n",
    "# net.initialize()\n",
    "# for layer in net:\n",
    "#     X = layer(X)\n",
    "#     print(layer.name,\"output shape: \\t\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据和训练模型\n",
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size = batch_size)\n",
    "\n",
    "def try_gpu():\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,),ctx = ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx \n",
    "\n",
    "ctx = try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalluate_accuracy(data_iter,net,ctx):\n",
    "    acc_sum, n = nd.array([0],ctx = ctx), 0\n",
    "    for X,y in data_iter:\n",
    "        # 如果ctx 代表GPU及相应的显存，将数据复制到显存上\n",
    "        X,y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis = 1) ==y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter,test_iter,batch_size, trainer, ctx, num_epochs):\n",
    "    \n",
    "    print(\"trainging on\",ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start = 0.0,0.0,0,time.time()\n",
    "       \n",
    "        for X,y in train_iter:\n",
    "            X,y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            \n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype(\"float32\")\n",
    "            \n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis = 1) ==y).sum().asscalar()\n",
    "            \n",
    "            n += y.size\n",
    "            \n",
    "        test_acc = evalluate_accuracy(test_iter, net, ctx)\n",
    "        print(\"epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec\" \n",
    "              %(epoch +1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainging on gpu(0)\n",
      "epoch 1, loss 2.3181, train acc 0.102, test acc 0.100, time 6.7 sec\n",
      "epoch 2, loss 1.9018, train acc 0.267, test acc 0.585, time 6.7 sec\n",
      "epoch 3, loss 0.9577, train acc 0.620, test acc 0.640, time 4.9 sec\n",
      "epoch 4, loss 0.7430, train acc 0.711, test acc 0.745, time 4.4 sec\n",
      "epoch 5, loss 0.6484, train acc 0.744, test acc 0.750, time 4.3 sec\n",
      "epoch 6, loss 0.6005, train acc 0.763, test acc 0.782, time 4.5 sec\n",
      "epoch 7, loss 0.5557, train acc 0.780, test acc 0.804, time 4.4 sec\n",
      "epoch 8, loss 0.5181, train acc 0.797, test acc 0.818, time 4.4 sec\n",
      "epoch 9, loss 0.4907, train acc 0.808, test acc 0.820, time 4.4 sec\n",
      "epoch 10, loss 0.4667, train acc 0.820, test acc 0.831, time 4.3 sec\n",
      "epoch 11, loss 0.4439, train acc 0.831, test acc 0.841, time 4.4 sec\n",
      "epoch 12, loss 0.4277, train acc 0.839, test acc 0.842, time 4.4 sec\n",
      "epoch 13, loss 0.4131, train acc 0.844, test acc 0.852, time 4.4 sec\n",
      "epoch 14, loss 0.4006, train acc 0.850, test acc 0.857, time 4.4 sec\n",
      "epoch 15, loss 0.3881, train acc 0.856, test acc 0.861, time 4.4 sec\n",
      "epoch 16, loss 0.3797, train acc 0.860, test acc 0.867, time 4.4 sec\n",
      "epoch 17, loss 0.3684, train acc 0.864, test acc 0.866, time 4.4 sec\n",
      "epoch 18, loss 0.3557, train acc 0.869, test acc 0.872, time 4.4 sec\n",
      "epoch 19, loss 0.3543, train acc 0.869, test acc 0.872, time 4.4 sec\n",
      "epoch 20, loss 0.3434, train acc 0.873, test acc 0.875, time 4.4 sec\n",
      "epoch 21, loss 0.3385, train acc 0.875, test acc 0.876, time 4.4 sec\n",
      "epoch 22, loss 0.3310, train acc 0.879, test acc 0.874, time 4.4 sec\n",
      "epoch 23, loss 0.3261, train acc 0.879, test acc 0.881, time 4.4 sec\n",
      "epoch 24, loss 0.3246, train acc 0.879, test acc 0.877, time 4.4 sec\n",
      "epoch 25, loss 0.3179, train acc 0.882, test acc 0.875, time 4.4 sec\n",
      "epoch 26, loss 0.3154, train acc 0.884, test acc 0.879, time 4.4 sec\n",
      "epoch 27, loss 0.3105, train acc 0.885, test acc 0.882, time 4.4 sec\n",
      "epoch 28, loss 0.3068, train acc 0.886, test acc 0.887, time 4.4 sec\n",
      "epoch 29, loss 0.3025, train acc 0.888, test acc 0.881, time 4.4 sec\n",
      "epoch 30, loss 0.2979, train acc 0.889, test acc 0.885, time 4.4 sec\n",
      "epoch 31, loss 0.2965, train acc 0.889, test acc 0.886, time 4.4 sec\n",
      "epoch 32, loss 0.2925, train acc 0.891, test acc 0.887, time 4.4 sec\n",
      "epoch 33, loss 0.2914, train acc 0.891, test acc 0.884, time 4.4 sec\n",
      "epoch 34, loss 0.2864, train acc 0.893, test acc 0.887, time 4.4 sec\n",
      "epoch 35, loss 0.2843, train acc 0.894, test acc 0.888, time 4.4 sec\n",
      "epoch 36, loss 0.2816, train acc 0.895, test acc 0.892, time 4.4 sec\n",
      "epoch 37, loss 0.2814, train acc 0.895, test acc 0.888, time 4.4 sec\n",
      "epoch 38, loss 0.2777, train acc 0.896, test acc 0.890, time 4.4 sec\n",
      "epoch 39, loss 0.2760, train acc 0.897, test acc 0.892, time 4.4 sec\n",
      "epoch 40, loss 0.2741, train acc 0.898, test acc 0.889, time 4.4 sec\n",
      "epoch 41, loss 0.2714, train acc 0.899, test acc 0.891, time 4.4 sec\n",
      "epoch 42, loss 0.2694, train acc 0.899, test acc 0.893, time 4.4 sec\n",
      "epoch 43, loss 0.2668, train acc 0.901, test acc 0.889, time 4.6 sec\n",
      "epoch 44, loss 0.2642, train acc 0.901, test acc 0.893, time 5.1 sec\n",
      "epoch 45, loss 0.2613, train acc 0.903, test acc 0.891, time 4.7 sec\n",
      "epoch 46, loss 0.2595, train acc 0.903, test acc 0.893, time 4.7 sec\n",
      "epoch 47, loss 0.2583, train acc 0.904, test acc 0.890, time 4.6 sec\n",
      "epoch 48, loss 0.2559, train acc 0.904, test acc 0.889, time 4.6 sec\n",
      "epoch 49, loss 0.2537, train acc 0.906, test acc 0.895, time 4.5 sec\n",
      "epoch 50, loss 0.2528, train acc 0.906, test acc 0.892, time 4.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs = 0.9,50\n",
    "net.initialize(force_reinit= True, ctx = ctx, init = init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
