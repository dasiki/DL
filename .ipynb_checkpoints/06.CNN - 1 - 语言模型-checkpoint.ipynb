{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **循环神经网络**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络：能有效处理空间信息\n",
    "\n",
    "循环神经网络：是为了更好地处理时序信息而设计的。 它引入状态变量来存储过去的信息，并用其与当前的输入共同决定当前的输出。\n",
    "\n",
    "循环神经网络用于处理序列数据，如一段文字或声音、购物或观影的顺序，甚至是图像中的一行或一列像素。因此，循环神经网络有着极为广泛的字应用，如语言模型、文本分类、机器翻译、语音识别、图像分析、手写识别和推荐系统。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **语言模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**语言模型（language model)** 是自然语言处理的重要技术。 自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。\n",
    "\n",
    "假设一段长度为 $T$ 的文本中的词依次为 $w_1,w_2,\\cdots,w_T$，那么在离散的时间序列中， $w_t(1\\leq t \\leq T)$可看作在时间步（time step) $t$ 的输出或标签。\n",
    "\n",
    "给定一个长度为 $T$ 的词的序列 $w_1,w_2,\\cdots,w_T$，语言模型将计算该序列的概率: $P(w_1,w_2,\\cdots,w_T)$。语言模型可用于提升语音识别和机器翻译的性能。 \n",
    "\n",
    "<font size=4 color='blue'>**1、语言模型的计算**</font>\n",
    "\n",
    "假设序列 $w_1,w_2,\\cdots,w_T$ 中的每个词是依次生成的，我们有 $$P(w_1,w_2,\\cdots,w_T) = \\prod_{t=1}^T P(w_t | w_1,\\cdots,w_{t-1})$$ \n",
    "例如，一段含有4个词的文本序列的概率：$$ P(w_1,w_2,w_3,w_4) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3）$$ \n",
    "为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。\n",
    "\n",
    "设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如，$P(w_1)$ 可以计算为 $w_1$ 在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。\n",
    "\n",
    "因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如，$P(w_2|w_1)$ 可以计算为 $w_1,w_2$ 两词相邻的频率与 $w_1$词频的比值，因为该比值即 $P(w_1,w_2)$ 与 $P(w_1)$ 之比； 而 $P(w_3|w_1,w_2)$ 同理可计算为 $w_1,w_2$ 和 $w_3$ 这3个词相邻的频率与 $w_1$ 和 $w_2$ 这2个词相邻的频率比例。以此类推。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>**2、n元语法**</font>\n",
    "\n",
    "当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设简化了语言模型的计算。\n",
    "\n",
    "这里的马尔可夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n )。如果 $n=1$，那么有 $P(w_3|w_1,w_2) = P(w_3|w_2)$。如果基于 $n-1$ 阶马尔可夫链，我们可以将语言模型改写为： $$ P(w_1,w_2,\\cdots,w_T) = \\prod_{t=1}^T P(w_t | w_{t - (n-1)},\\cdots,w_{t-1})$$ \n",
    "\n",
    "以上也叫 n 元语法（n-grams)。 它是基于 $n-1$ 阶马尔可夫链的概率语言模型。当 $n$ 分别为1，2，3时，我们将其分别称作一元语法（unigram)、二元语法（bigram)和三元语法（trigram)。\n",
    "\n",
    "例如，长度为4的序列 $w_1,w_2,w_3,w_4$在一元语法、二元语法和三元语法中的概率分别为： \n",
    "#### $$ \\begin{align} & P(w_1,w_2,w_3,w_4) = P(w_1)P(w_2)P(w_3)P(w_4) \\\\\n",
    "                      & P(w_1,w_2,w_3,w_4) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3)\\\\\n",
    "                      & P(w_1,w_2,w_3,w_4) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)\\end{align}$$ \n",
    "当 n 较小时，n元语法往往不准确。例如，在一元语法中，由三个词组成的句子 \"你走先\" 和 \"你先走\" 的概率是一样的。然而，当n较大时，n元语法需要计算并存储大量的词频和多词相邻频率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
