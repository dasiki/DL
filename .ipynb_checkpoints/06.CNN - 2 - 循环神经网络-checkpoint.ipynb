{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **循环神经网络**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节的$n$元语法中，时间步 $t$ 的词 $w_t$ 基于前面所有词的条件概率只考虑了最近时间步的 $n-1$词。如果要考虑比 $t - (n-1) $ 更早时间步的词对 $w_t$ 的可能影响，我们需要增大 $n$ 。 但是这样模型参数的数量将随之呈指数级增长。\n",
    "\n",
    "本章介绍循环神经网络。**它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**1、不含隐藏状态的神经网络**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一个含单隐藏层的多层感知机。给定样本数 $n$、输入个数（特征数或特征向量维度）为 $d$ 的小批量数据样本 $X\\in \\mathcal{R}^{n\\times d}$。设隐藏层的激活函数为 $\\phi$，那么隐藏层的输出 $\\mathcal{H}\\in \\mathcal{R}^{n\\times h}$ 计算为\n",
    "####  $$H = \\phi(XW_{xh}) + b_h$$ \n",
    "其中，隐藏层权重 $XW_{xh} \\in \\mathcal{R}^{d\\times h}$，隐藏层偏差参数 $b_h \\in \\mathcal{R}^{1\\times h}$， $h$ 为隐藏层单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量 $H$ 作为输出层的输入，且设输出个数为 $q$（如分类问题中的类别数），输出层的输出为 \n",
    "#### $$O = HW_{hq} + b_q$$\n",
    "其中，输出变量 $O\\in \\mathcal{R}^{n\\times q}$，输出层权重参数 $W_{hq}\\in \\mathcal{R}^{h\\times q}$，输出层偏差参数 $b_q \\in \\mathcal{R}^{1\\times q}$。如果是分类问题，我们可以使用softmax(O)来计算输出类别的概率分类。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**2、含隐藏状态的神经网络**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们考虑输入数据存在时间相关性的情况。假设 $X_t \\in\\mathcal{R}^{n\\times d}$是时间序列中时间步 $t$ 的小批量输入， $H_t \\in\\mathcal{R}^{n\\times h}$ 是该时间步的隐藏变量层。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $H_{t-1}$，并引入一个新的权重参数 $W_{hh}\\in \\mathcal{R}^{h\\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步 $t$ 的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "#### $$ H_t = \\phi(X_tW_{th} + H_{t-1}W_{hh} + b_{h})$$ \n",
    "与多层感知机相比，这里添加了$H_{t-1}W_{hh}$一项。由上式中相邻时间步的隐藏变量 $H_t$ 和 $H_{t-1}$ 之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network)。\n",
    "\n",
    "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中的隐藏状态的循环计算。在时间步 $t$，输出层的输出和多层感知机中的计算类似：\n",
    "#### $$O_t = H_tW_{tq} + b_q $$\n",
    "循环神经网络的参数包括隐藏层的权重 $W_{xh}\\in \\mathcal{R}^{d\\times h}$、 $W_{hh}\\in \\mathcal{R}^{h\\times h}$ 和偏差 $b_h\\in \\mathcal{R}^{1\\times h}$，以及输出层的权重 $W_{hq}\\in \\mathcal{R}^{h\\times q}$ 和偏差 $b_q\\in \\mathcal{R}^{1\\times q}$。**即使在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。** \n",
    "\n",
    "图6.1展示了循环神经网络在3个相邻时间步的计算逻辑。隐藏状态的可以看成是输入 $X_t$ 和前一时间步隐藏状态 $H_{t-1}$ 连结后输入一个激活函数为 $\\phi$ 的全连接层。该全连接层的输出就是当前时间步的隐藏状态 $H_t$，且模型参数为 $W_{xh}$ 与 $W_{hh}$ 的连结，偏差为 $b_h$。当前时间步 $t$ 的隐藏状态 $H_t$ 将参与下一个时间步 $t+1$ 的隐藏状态 $H_{t+1}$的计算，并输入到当前时间步的全连接输出层。\n",
    "\n",
    "**隐藏状态中 $X_tW_{xh} + H_{t-1}W_{hh}$ 的计算等价于 $X_t$ 与 $H_{t-1}$连结后的矩阵乘以 $W_{xh}$ 与 $W_{hh}$连续后的矩阵。**\n",
    "\n",
    "#### $\\Rightarrow X_tW_{xh} + H_{t-1}W_{hh} = \\big[X_t , H_{t-1} \\big] \\bullet \\left[\\begin{align} & W_{xh} \\\\ & W_{hh} \\end{align}\\right]$,  $\\quad X_t \\in\\mathcal{R}^{n\\times d}$，$ W_{xh}\\in \\mathcal{R}^{d\\times h}$， $H_t \\in\\mathcal{R}^{n\\times h}$ , $W_{hh}\\in \\mathcal{R}^{h\\times h}$\n",
    "<img src = \"cnn.png\" width = 600 height = 300></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证： 隐藏状态中 $X_tW_{xh} + H_{t-1}W_{hh}$ 的计算等价于 $X_t$ 与 $H_{t-1}$连结后的矩阵乘以 $W_{xh}$ 与 $W_{hh}$连续后的矩阵。\n",
    "\n",
    "$X_t \\in\\mathcal{R}^{n\\times d}$，$ W_{xh}\\in \\mathcal{R}^{d\\times h}$， $H_t \\in\\mathcal{R}^{n\\times h}$ , $W_{hh}\\in \\mathcal{R}^{h\\times h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951559  -7.028843    6.2385654   3.5568771 ]\n",
       " [ 2.8098507  -1.8081224   0.6729959  -0.23211236]\n",
       " [-0.14438525 -2.5961134  -1.1423199  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "X,W_xh = nd.random.normal(shape=(3,1)), nd.random.normal(shape = (1,4))\n",
    "H,W_hh = nd.random.normal(shape=(3,4)), nd.random.normal(shape = (4,4))\n",
    "\n",
    "nd.dot(X,W_xh) + nd.dot(H,W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将矩阵X和H按列(维度1)连结，连结后的矩阵形状为（3，5）。然后将矩阵 $W_{xh}$ 和 $W_{hh}$ 按行（维度0）连结，连结后的矩阵形状为（5，4）。最后将两个连结点的矩阵相乘，得到与上面代码输出相同的形状为（3，4）的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 3.1951559  -7.028843    6.2385654   3.5568771 ]\n",
       " [ 2.8098507  -1.8081224   0.6729959  -0.23211236]\n",
       " [-0.14438534 -2.5961137  -1.1423199  -4.142916  ]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(nd.concat(X,H,dim = 1),nd.concat(W_xh,W_hh, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>**3、应用：基于字符级循环神经网络的语言模型**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设小批量中样本数为1，文本序列为\"想\"\"要\"\"有\"\"直\"\"升\"\"机\"。图6.2演示了如何使用循环神经网络基于当前和过去的字符来预测下一个字符。在训练时，我们对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算它与标签的误差。在图6.2中，由于隐藏层中隐藏状态的循环计算，时间步3的输出 $O_3$ 取决于文本序列 \"想\"\"要\"\"有\"。由于训练数据中该序列的下一个词为\"直\"，时间步3的损失将取决于该时间步基于序列\"想\"\"要\"\"有\"\"生成下一个词的概率分布与该时间步的标签 \"直\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn2.png\" width = 800 ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每个输入词是一个字符，因此这个模型被称为字符级循环神经网络（character-level recurrent neural network)。因为不同字符的个字远小于不同词的个数，所以字符级循环神经网络的计算通常更加简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
