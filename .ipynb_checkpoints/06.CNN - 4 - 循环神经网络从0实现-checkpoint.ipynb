{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **循环神经网络从0实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def load_data_jay_lyrics():\n",
    "    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\n",
    "    with zipfile.ZipFile('../d2l-zh/data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 1、读取数据\n",
    "import d2lzh as d2l\n",
    "import math \n",
    "from mxnet import autograd,nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time  \n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char,vocab_size) = load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x1027 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### 2、one-hot向量\n",
    "### 为了将词表示成向量输入到神经网络，一个简单的方法是使用one-hot向量。\n",
    "### 假设词典中不同字符的数量为N(即词典大小 vocab_size)，每个字符已经同一个从 0 到 N-1 的连续整数值索引一一对应。如果一个字符的索引是整数i,那么，我们创建\n",
    "### 一个全0的长为N的向量，并将其位置为 i 元素设为1 。 该向量就是对原字符的 one-hot向量。\n",
    "\n",
    "# 例 索引为0和2的one-hot向量，长度 N = vocab_size\n",
    "nd.one_hot(nd.array([0,2]), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每次采样的小批量形状是 (批大小，时间步)。\n",
    "# 下面的函数将这样的小批量变换成数个可以输入进网络的形状为（批大小，词典大小）的矩阵。\n",
    "# 即 时间步 t 的输入为 nXd,其中，n为批大小，d为输入个数，即one-hot向量长度（词典大小）\n",
    "\n",
    "def to_onehot(X,size): \n",
    "    return [nd.one_hot(x,size) for x in X.T]\n",
    "\n",
    "x = nd.arange(20).reshape((2,5))\n",
    "inputs = to_onehot(x, vocab_size)\n",
    "len(inputs), inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "############## 3、初始化模型参数\n",
    "num_inputs,num_hiddens,num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = d2l.try_gpu()\n",
    "print(\"will use\",ctx)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale = 0.01, shape = shape, ctx = ctx)\n",
    "    \n",
    "    # 隐藏层参数\n",
    "    w_xh = _one((num_inputs,num_hiddens))\n",
    "    w_hh = _one((num_hiddens,num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx = ctx)\n",
    "    \n",
    "    # 输出层参数\n",
    "    w_hq = _one((num_hiddens,num_outputs))\n",
    "    b_q = nd.zeros(num_outputs,ctx = ctx)\n",
    "    \n",
    "    # 附上梯度\n",
    "    params = [w_xh, w_hh, b_h, w_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 4.定义模型\n",
    "# 定义init_rnn_state函数来返回初始化的隐藏状态 ,返回值为（批大小，隐藏单元个数）\n",
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape = (batch_size, num_hiddens), ctx = ctx), )\n",
    "\n",
    "# 下面的rnn函数定义了在一个时间步里如何计算隐藏状态和输出\n",
    "def rnn(inputs, state, params):\n",
    "    # input,output 皆为num_steps 个形状为(batch_size, vocab_size) 的矩阵\n",
    "    w_xh, w_hh,b_h,w_hq, b_q = params \n",
    "    H, = state\n",
    "    outputs = []\n",
    "    \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,w_xh) + nd.dot(H,w_hh) + b_h) ## nd.dot(H,w_hh) : H为上一个状态的隐藏状态\n",
    "        Y = nd.dot(H,w_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,(H,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), (2, 256))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "state = init_rnn_state(x.shape[0], num_hiddens, ctx)\n",
    "inputs = to_onehot(x.as_in_context(ctx), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ 5.定义预测函数\n",
    "# 以下函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符\n",
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    \n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(nd.array([output[-1]], ctx = ctx), vocab_size)\n",
    "        \n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        \n",
    "        # 下一个时间步的输入是prefix里的字符或当前最佳预测字符\n",
    "        if t<len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis = 1).asscalar()))\n",
    "    return \"\".join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开马鹿久感马纵躲灰试征'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  test\n",
    "predict_rnn(\"分开\",10,rnn,params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **裁剪梯度**\n",
    "\n",
    "循环神经网络中容易出来梯度衰减或梯度爆炸。为了应对梯度梯度爆炸，我们可以裁剪梯度（clip gradient)。假设我们把所有模型参数的梯度元素拼接成一个向量$g$，并设裁剪的阈值是$\\theta$，裁剪后的梯度\n",
    "#### $$min\\big(\\frac{\\theta}{||g||},1\\big)g$$\n",
    "的$L_2$ 范数不超过$\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params,theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad **2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 未完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
