{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **全局向量的词嵌入(GloVe)**\n",
    "\n",
    "[1] Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>**1、全局向量的词嵌入(GloVe)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec中的跳字模型：** 将跳字模型中使用softmax运算表达的条件概率 $P(w_j|w_i)$ 记作 $q_{ij}$，即\n",
    "#### $$ q_{ij} = \\frac{exp(u_j^T v_i)}{\\sum_{k\\in\\mathcal{V}} exp(u_k^Tv_i)}$$ \n",
    "其中， $v_i$ 和 $u_i$ 分别是索引为 $i$ 的词 $w_i$ 作为中心词和背景词时的向量表示， $\\mathcal{V} = \\{0,1,\\cdots,|\\mathcal{V}| - 1\\}$ 为词典索引集。\n",
    "\n",
    "对于词 $w_i$, 它在数据集中可能多次出来。我们将每一次以它作为中心词的所有背景词全部汇总并保留重复元素，记作多重集(multiset) $C_i$。一个元素在多重集中的个数称为该元素的重数(multiplicity)。举例来说，假设词 $w_i$ 在数据集中出现2次：文本序列中以这2个 $w_i$ 作为中心词的背景窗口分别包含背景索引2,1,5,2 和 2,3,2,1。那么，多重集 $C_i = {1,1,2,2,2,2,3,5}$ ，其中，元素1的重数为2，元素2的重数为4,元素3,5 的重数均为1。将多重集 $C_i$ 中元素 $j$ 的重数记作 $x_{ij}$。它表示了整个数据集中的所有以 $w_i$ 为中心词的背景窗口中词 $w_j$ 的个数。那么，跳字模型的损失函数还可以用另一种方式表达： \n",
    "#### $$ - \\sum_{i\\in \\mathcal{V}}\\sum_{j\\in \\mathcal{V}} x_{ij}log q_{ij}$$ \n",
    "\n",
    "我们将数据集中所有以词 $w_i$ 为中心词的背景词的数量之和 $|C_i|$ 记为 $x_i$，并将以 $w_i$ 为中心词生成背景词 $w_j$ 的条件概率 $w_{ij}/x_i$ 记作 $p_{ij}$。我们可以进一步将跳字模型的损失函数改写为\n",
    "#### $$ - \\sum_{i\\in \\mathcal{V}}x_i\\sum_{j\\in \\mathcal{V}} p_{ij}log q_{ij}$$ \n",
    "上式中，$\\sum_{j\\in \\mathcal{V}} p_{ij}log q_{ij}$ 计算的是以 $w_i$ 为中心词的背景词条件概率分布 $p_{ij}$ 和模型预测的条件概率分布 $q_{ij}$ 的交叉熵，且损失函数使用所有以词 $w_i$ 为中心词的背景词的数量之和来加权。最小化上式中的损失函数会令预测的条件概率分布尽可能接近真实的条件概率分布。\n",
    "\n",
    "然而，作为常用损失函数的一种，交叉熵损失函数有时并不是好的选择。一方面，令模型预测 $q_{ij}$ 成为合法概率分布的代价是它在分母中基于整个词典的累加项。这很容易带来过大的计算开销。另一方面，词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) GloVe模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe模型采用了平方损失，并 基于该损失对跳字模型做了3点改动：\n",
    "\n",
    "1） 使用非概率分布的变量 $p'_{ij} = x_{ij}$ 和 $q_{ij} = exp(u_j^Tv_i)$ 并对它们取对数。因此，平方损失项是 \n",
    "#### $$ \\big(log p'_{ij} - log q'_{ij}\\big)^2 = \\big(u_j^Tv_i - logx_{ij}\\big)^2$$ \n",
    "2） 为每个词 $w_i$ 增加两个为标量的模型参数：中心词偏差项 $b_i$ 和背景词偏差项 $c_i$ \n",
    "3） 将每个损失项的权重替换成函数 $h(x_{ij})$。权重函数 $h(x)$ 是值域在 [0,1] 的单调递增函数。\n",
    "\n",
    "如此一来，GloVe模型的目标是最小化损失函数：\n",
    "#### $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\begin{align}  & \\mathop{min}\\limits_{w,b}& & \\frac{1}{2}(w_1^2 + w_2^2)\\\\\n",
    "                      & s.t. &                     & 3w_1 + 3w_2 + b\\geq 1\\\\\n",
    "                      &      &                     & 4w_1 + 3w_2 + b\\geq 1\\\\\n",
    "                      &      &                     & -w_1 -w_2 - b\\geq 1\\end{align}$$ \n",
    "                                               \n",
    "**拉格郎日函数:** $$ L(w_1 ,w_2 , b,\\alpha_1,\\alpha_2,\\alpha_3) = \\frac{1}{2}(w_1^2 + w_2^2) + \\alpha_1(3w_1 + 3w_2 + b - 1) + \\alpha_2(4w_1 + 3w_2 + b - 1) + \\alpha_3(-w_1 - w_2 - b - 1)$$ \n",
    "\n",
    "#### $\\mathop{max}\\limits_{\\alpha_1,\\alpha_2,\\alpha_3}\\mathop{min}\\limits_{w_1 ,w_2 , b}  L(w_1 ,w_2 , b,\\alpha_1,\\alpha_2,\\alpha_3)$  ：\n",
    "\n",
    "$$\\left\\{\\begin{align}\\frac{\\partial L}{\\partial w_1} = w_1 + 3\\alpha_1 + 4\\alpha_2 - \\alpha_3 = 0 \\Rightarrow w_1 = - 3\\alpha_1 - 4\\alpha_2 + \\alpha_3 \\\\\n",
    "                           \\frac{\\partial L}{\\partial w_2} = w_2 + 3\\alpha_1 + 3\\alpha_2 - \\alpha_3 = 0 \\Rightarrow w_2 = - 3\\alpha_1 - 3\\alpha_2 + \\alpha_3 \\\\\n",
    "                            \\frac{\\partial L}{\\partial b} = \\alpha_1 + \\alpha_2 - \\alpha_3 = 0\\end{align}\\right.$$\n",
    "                            \n",
    "#### $\\begin{align}& L(w_1 ,w_2 , b,\\alpha_1,\\alpha_2,\\alpha_3) \\\\\n",
    "                    &= \\frac{1}{2}(- 3\\alpha_1 - 4\\alpha_2 + \\alpha_3)^2 + \\frac{1}{2}(- 3\\alpha_1 - 3\\alpha_2 + \\alpha_3)^2 \\\\\n",
    "                    & + \\alpha_1(-18\\alpha_1 - 21\\alpha_2 + 6\\alpha_3 + b - 1) + \\alpha_2(-21\\alpha_1 - 25\\alpha_2 + 7\\alpha_3 + b - 1) + \\alpha_3(6\\alpha_1 + 7\\alpha_2 - 2\\alpha_3 - b - 1) \\\\\n",
    "                    &= \\frac{1}{2}(- 3\\alpha_1 - 4\\alpha_2 + \\alpha_3)^2 -18\\alpha_1^2 - 25\\alpha_2^2 -2\\alpha_3^2 -42\\alpha_1\\alpha_2 + 12 \\alpha_1\\alpha_3 + 14\\alpha_2\\alpha_3 + (b-1)\\alpha_1 + (b-1)\\alpha_2 - (b+1)\\alpha_3 \\end{align}$ \n",
    "\n",
    "#### $\\begin{align}&\\frac{\\partial L}{\\partial \\alpha_1} = \\end{align}$\n",
    "$$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
