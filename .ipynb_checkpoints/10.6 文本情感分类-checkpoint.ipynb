{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata,loss as gloss,nn,rnn,utils as gutils\n",
    "import os \n",
    "import random\n",
    "import tarfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "\n",
    "def download_imdb(data_dir = \"../data/\"):\n",
    "    # url = (\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
    "    # sha1 = \"01ada507287d82875905620988597833ad4e0903\"\n",
    "    # fname = gutils.download(url, data_dir, sha1_hash=sha1)\n",
    "    fname = \"../data/aclImdb_v1.tar.gz\"\n",
    "    \n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(data_dir)\n",
    "        \n",
    "download_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练数据集和测试数据集；每个样本是一条评论以及其对应的标签：1：正面；0：负面\n",
    "def read_imdb(folder = 'train') :\n",
    "    data = []\n",
    "    for label in ['pos','neg']:\n",
    "        folder_name = os.path.join(\"../data/aclImdb/\",folder, label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode(\"utf-8\").replace(\"\\n\",\"\").lower()\n",
    "                data.append([review ,1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "        \n",
    "train_data,test_data = read_imdb(\"train\"),read_imdb(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('# words in vocab:', 46152)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预处理数据\n",
    "\n",
    "def get_tokenized_imdb(data): \n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(\" \")]\n",
    "    return [tokenizer(review) for review,_ in data]\n",
    "\n",
    "# 创建词典\n",
    "def get_vocab_imdb(data):\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return text.vocab.Vocabulary(counter, min_freq = 5, reserved_tokens = ['<pad>'])\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "\n",
    "'# words in vocab:', len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于每条评论长度不一，不以直接组合成小批量\n",
    "# 通过词典转换成词索引，然后通过截断或补 '<pad>' (padding) 符号来将每条评论长度固定为500\n",
    "\n",
    "def preprocess_imdb(data,vocab):\n",
    "    max_l = 500 \n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x + [vocab.token_to_idx['<pad>']] * (max_l - len(x))\n",
    "    \n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\n",
    "    labels = nd.array([score for _,score in data])\n",
    "    \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据迭代器，每次迭代一个小批量的数据\n",
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(*preprocess_imdb(train_data,vocab))\n",
    "test_set = gdata.ArrayDataset(*preprocess_imdb(test_data, vocab))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle = True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 500) y (64,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('# batches:', 391)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X,y in train_iter:\n",
    "    print(\"X\",X.shape, \"y\",y.shape)\n",
    "    break\n",
    "    \n",
    "\"# batches:\",len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用循环神经网络的模型\n",
    "# 在这个模型中，每个词先通过嵌入层得到特征向量。\n",
    "# 然后，使用双向循环神经网络对特征序列进一步编码得到序列信息。\n",
    "# 最后，将编码的序列信息通过全连接层变换为输出；\n",
    "# 具体地，将双向长短期记忆在最初时间步和最终时间步的隐藏状态连结，作为特征序列的表征传递给输出层分类。\n",
    "\n",
    "class BiRNN(nn.Block):\n",
    "    def __init__(self,vocab,embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # bidirectional 设为True 即得到双向循环神经网络\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers = num_layers, bidirectional = True, input_size = embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs 的形状是(批大小，词数), 因为LSTM 需要将序列作为第一维，所以将输入转置后再提取词特征，输出形状为（词数，批大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # rnn.LSTM 只传入输入embeddings, 因此只返回最后一层的隐藏层在各时间步的隐藏状态\n",
    "        # outputs 形状是（词数，批大小，2*隐藏单元个数）\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入，它的形状是 （批大小，4*隐藏单元个数）\n",
    "        encoding = nd.concat(outputs[0], outputs[-1])\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个含两个隐藏层的双向循环神经网络\n",
    "embed_size , num_hiddens, num_layers, ctx = 100,100,2,d2l.try_all_gpus()\n",
    "net = BiRNN(vocab,embed_size, num_hiddens, num_layers)\n",
    "net.initialize(init.Xavier() ,ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\86150\\AppData\\Roaming\\mxnet\\embeddings\\glove\\glove.6B.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/glove/glove.6B.zip...\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练的词向量\n",
    "glove_embedding = text.embedding.create('glove', pretrained_file_name = 'glove.6B.100d.txt',vocabulary = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将用这些词向量作为评论中每个词的特征向量。\n",
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "net.embedding.collect_params().setattr(\"grad_req\", \"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "lr,num_epochs = 0.01,5\n",
    "trainer = gluon.Train(net.collect_params(),\"adam\",{\"learning_rate\":lr})\n",
    "loss = gloss.SoftmaxEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
