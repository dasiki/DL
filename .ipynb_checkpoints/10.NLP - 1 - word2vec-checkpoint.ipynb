{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **词嵌入（word2vec）**\n",
    "\n",
    "自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，**词向量是用来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌入(word embedding)**。 近年来，词嵌入已经逐渐成为自然语言处理的基础知识。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、为何不采用one-hot向量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**one-hot向量表示词**：假设词典中不同词的数量（词典大小）为 $N$，每个词可以和从 0 到$N-1$ 的连续整数一一对应。这些与词对应的整数叫作词的索引。假设一个词的索引为 $i$，为了得到该词的one-hot向量表示，我们创建一个全0的长为 $N$ 的向量，并将第 $i$ 位设为1。这样一来，每个词就表示成了一个长为 $N$ 的向量，可以直接被神经网络使用。\n",
    "\n",
    "虽然one-hot词向量构造起来很容易，但是通常并不是一个好选择。一个主要的原因是，one-hot词向量无法准确表达不同词之间相似度，如我们常用的余弦相似度。对于向量 $x,y\\in\\mathcal{R}^d$，它们的余弦相似度是它们之间的夹角的余弦值：\n",
    "##### $$ \\frac{x^Ty}{||x||||y||}\\in [-1,1]$$ \n",
    "由于任何两个不同词的one-hot向量的余弦相似度都是0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。\n",
    "\n",
    "**word2vec工具的提出正是为了解决上面的这个问题，它将每个词表示成一个定长的向量，并使用这些向量能较好地表达不同词之间的相似和类比关系。**\n",
    "\n",
    "word2vec工具包含了两个模型，即跳字模型(skip-gram)和连结词袋模型（continuous bag of words,CBOW）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、跳字模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**跳字模型假设基于某个词来生成它在文本序列周围的词**。例如，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.2所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son” 的条件概率，即\n",
    "##### $$P(\"the\",\"man\",\"his\",\"son\"|\"loves\")$$\n",
    "假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成\n",
    "##### $$P(\"the\"|\"loves\") P(\"man\"|\"loves\")P(\"his\"|\"loves\")P(\"son\"|\"loves\")$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='word2vec1.png' width = 300></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在跳字模型中，每个词被表示为两个 $d$ 维向量，用来计算条件概率。假设这个词在词典中索引为 $i$，当它为中心词时向量表示为 $v_i\\in \\mathcal{R}^d$，而为背景词时向量表示为 $u_i\\in \\mathcal{R}^d$。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：\n",
    "#### $$ P(w_o|w_c) = \\frac{exp(u_o^Tv_c)}{\\sum_{i\\in \\mathcal{V}}exp(u_i^Tv_c)}$$ \n",
    "其中，词典索引 $\\mathcal{V} = \\{0,1,\\cdots,|\\mathcal{V}|-1\\}$。假设给定一个长度为 $T$ 的文本序列，设时间步 $t$ 的刻画 为 $w^{(t)}$。假设给定中心词的情况 下背景词的生成相互独立，当背景窗口大小为 $m$ 时，跳字模型的似然函数即给定任一中心词生成所有背景词概率 \n",
    "#### $$ \\prod_{t=1}^T\\prod_{-m\\leq j\\leq m,j\\neq 0}P(w^{(t+j)}|w^{(t)})$$\n",
    "这里，小于1或大于 $T$ 的时间步可以被忽略。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 训练跳字模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跳字模型的参数是每个词所对应的中心词向量和背景词向量。\n",
    "\n",
    "训练中我们通过最大似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：\n",
    "#### $$ - \\sum_{t=1}^T\\sum_{-m\\leq j\\leq m,j\\neq 0}logP(w^{(t+j)}|w^{(t)})$$\n",
    "如果使用随机梯度下降，那么在第一次迭代里面我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。首先看到\n",
    "#### $$ logP(w_o|w_c) = u_o^Tv_c - log\\big(\\sum_{i\\in \\mathcal{V}}exp(u_i^Tv_c)\\big)$$ \n",
    "通过微分，我们可以得到上式中$v_c$的梯度 \n",
    "#### $$ \\frac{\\partial logP(w_o|w_c)}{\\partial v_c} = u_o^T - \\frac{\\sum_{j\\in \\mathcal{V}}exp(u_j^Tv_c)u_j}{\\sum_{i\\in \\mathcal{V}}exp(u_i^Tv_c)} = u_o^T - \\sum_{j\\in\\mathcal{V}}\\big(\\frac{exp(u_j^Tv_c)}{\\sum_{i\\in \\mathcal{V}}exp(u_i^Tv_c)}\\big)u_j = u_o^T - \\sum_{j\\in\\mathcal{V}}P(w_j|w_c)u_j$$ \n",
    "它的计算需要词典中所有词以w_c为中心词的条件概率。有关其他词的梯度同理可得。\n",
    "\n",
    "训练结束后，对于词典中任一索引为 $i$ 的词，我们均得到该词作为中心词和背景词的两组向量 $v_i$ 和 $u_i$ 。在自然语言处理应该中，一般使用跳词模型的中心词向量作为词的表征向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、连续词袋模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，**连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词**。在同样的文本序列“the”“man”“loves”“his”“son”里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词“the”“man”“his”“son” 生成中心词 “loves”的条件概率（如图10.2所示）。也就是\n",
    "#### $$P(\"loves\"|\"the\",\"man\",\"his\",\"son\") $$ \n",
    "\n",
    "因为连续词袋模型的背景词有多个，我们将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。设 $v_i\\in \\mathcal{R}^d$ 和 $u_i\\in \\mathcal{R}^d$ 分别表示词典中索引为 $i$ 的词作为背景词和中心词的向量（这里符号的含义与跳字模型中相反）。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_{o_1},\\cdots,w_{o_{2m}}$在词典中索引为 $o_1,\\cdots,o_{2m}$，那么给定背景词生成中心词的条件概率\n",
    "#### $$ P(w_c|w_{o_1},\\cdots,w_{o_{2m}}) = \\frac{exp\\big(\\frac{1}{2m}u_c^T(v_{o1} + \\cdots,v_{o2m})\\big)}{\\sum_{i\\in\\mathcal{V}}exp\\big(\\frac{1}{2m}u_i^T(v_{o1} + \\cdots,v_{o2m})\\big)} $$ \n",
    "为了让符号更简单，记 $W_o = \\{w_{o_1},\\cdots,w_{o_{2m}}\\}$ ，且$\\bar{v}_o = (v_{o1} + \\cdots,v_{o2m})/(2m)$，那么 ，上式可简写为\n",
    "#### $$ P(w_c|W_o) = \\frac{exp(u_c^T\\bar{v}_o)}{\\sum_{i\\in\\mathcal{V}}exp(u_i^T\\bar{v}_o)}$$ \n",
    "给定一个长度为 $T$ 的文本序列，设时间 $t$ 的词为 $w^{(t)}$，背景窗口大小为 $m$。连续词袋模型的似然函数是由背景词生成任一中心词的概率\n",
    "#### $$ \\prod_{t=1}^T P(w^{(t)}|w^{(t-m)},\\cdots,w^{(t-1)},w^{(t+1)},\\cdots,w^{(t+m)} )$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"word2vec2.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1、训练连续词袋模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练连续词袋模型同训练跳字模型基本一样。连续词袋模型的最大依然估计等价于最小化损失函数\n",
    "#### $$ -\\sum_{t=1}^T P(w^{(t)}|w^{(t-m)},\\cdots,w^{(t-1)},w^{(t+1)},\\cdots,w^{(t+m)} )$$ \n",
    "注意到，\n",
    "#### $$ logP(w_c|W_o) = u_c^T\\bar{v}_o - log\\sum_{i\\in\\mathcal{V}}exp(u_i^T\\bar{v}_o)$$ \n",
    "通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量 $v_{oi}(i = 1,2,\\cdots,2m)$ 的梯度\n",
    "#### $$ \\frac{\\partial logP(w_c|W_o)}{\\partial v_{oi}} = \\frac{1}{2m}\\big(u_c - \\sum_{j\\in\\mathcal{V}}\\frac{exp(u_j^T\\bar{v}_o)u_j}{\\sum_{i\\in\\mathcal{V}}exp(u_i^T\\bar{v}_o)}\\big) = \\frac{1}{2m}\\big( u_c - \\sum_{j\\in\\mathcal{V}}P(w_j|W_o)u_j\\big)$$ \n",
    "有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，我们一般使用连续词袋模型的背景词向量作为词的表征向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
