{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **多层感知机(multilayer perceptron, MLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1、隐藏层**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"hidelayer.png\" width = 600 height = 300></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个小批量样本 $X\\in \\mathcal{R}^{n\\times d}$，其批量大小为 $n$，输入个数为 $d$；假设多层感知机只有一个隐藏层，其中隐藏层单元个数为 $h$ 。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为 $H, H\\in \\mathcal{R}^{n\\times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为 $W_h \\in \\mathcal{R}^{n\\times h}, b_h\\in \\mathcal{R}^{1\\times h}$ ；输出层的权重和偏差参数分别为 $W_o \\in \\mathcal{R}^{h\\times q}, b_h\\in \\mathcal{R}^{1\\times q}$ \n",
    "\n",
    "输出 $O\\in \\mathcal{R}^{n\\times q}$ 的计算为：\n",
    "#### $H = XW_h + b_h \\\\ O = HW_o + b_o$ \n",
    "\n",
    "也就是将隐藏层的输出直接作为输出层的输入。如果将上式联立起来，可得：\n",
    "#### $O = (XW_h + b_h)W_o + b_o = XW_hW_o + b_hW_o + b_o$ \n",
    "\n",
    "可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：输出层权重参数为：$W_hW_o$，偏差参数为 $b_hW_o + b_o$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2、激活函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation)，而多个仿射变换的叠加仍然是一个仿射变换。\n",
    "\n",
    "解决方法是引入非线性变换，例如对隐藏层变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数称为激活函数（activation function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RELU函数：**  <font size=4> $Relu(x) = max(x,0)$</font> 只保留正数元素，将负数元素清零。\n",
    "\n",
    "**Sigmoid函数：**  <font size=4> $sigmoid(x) = \\frac{1}{1+exp(-x)}$ </font> 将元素的值变换为0-1之间。当输入越接近0时，sigmoid函数越接近线性变换。\n",
    "\n",
    "**tanh函数：** <font size=4> $tanh(x) = \\frac{1 - exp(-2x)}{ 1+ exp(-2x)} $</font> 将元素的值变换为 -1 到 1之间。该函数形状与sigmoid函数很像，但是tanh函数在坐标系的原点上对称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "def activation(x,name):\n",
    "    if name == 'relu':\n",
    "        return nd.array([max(i.asscalar(),0) for i in x])\n",
    "    if name == 'sigmoid':\n",
    "        return 1/(1 + nd.exp( -x ))\n",
    "    if name == 'tanh':\n",
    "        return (1 - nd.exp(-2*x)) / (1 + nd.exp(-2*x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd,nd \n",
    "\n",
    "def xyplot(x_val,y_vals,name):\n",
    "    d2l.set_figsize(figsize = (5,2.5))\n",
    "    d2l.plt.plot(x_val.asnumpy(), y_vals.asnumpy())\n",
    "    d2l.plt.xlabel(\"x\")\n",
    "    d2l.plt.ylabel(name + \"(x)\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.arange(-8.0,8.0,0.1)\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
