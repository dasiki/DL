{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>**1、正向传播**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正向传播（forward propagation)** 是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。\n",
    "\n",
    "假设输入是一特征为 $x\\in \\mathcal{R}^d$ 的样本，且不考虑偏差项，那么中间变量 $z = W^{(1)}x$，  其中，$W^{(1)} \\in \\mathcal{R}^{h\\times d}$ 是隐藏层的权重参数。  \n",
    "\n",
    "把中间变量 $z\\in \\mathcal{R}^h$ 输入按元素运算的激活函数 $\\phi$ 后，将得到列向量长度为 $h$ 的隐藏层变量 $h = \\phi(z)$ 隐藏层变量 $h$ 也是中间变量。\n",
    "\n",
    "假设输出层参数只有权重 $W^{(2)}\\in \\mathcal{R}^{q\\times h}$ ，可以得到向量长度为 $q$ 的输出层变量 $o = W^{(2)}h$ \n",
    "\n",
    "假设损失函数为 $\\mathcal{l}$，且样本标签为 $y$，可以计算出单个数据样本的损失项 $L = \\mathcal{l}(o,y)$\n",
    "\n",
    "根据 $L_2$ 范数正则化的定义，正则化项 <font size=4> $s = \\frac{\\lambda}{2}\\big(||W^{(1)}||_F^2 + ||W^{(2)}||_F^2\\big)$</font>，其中矩阵的Frobenius范数等价于将矩阵变平后计算 $L_2$ 范数。\n",
    "\n",
    "**模型在给定的数据样本上带正则化的损失为 $J = L + s$，我们将 $J$ 称为有关给定数据样本的目标函数** \n",
    "\n",
    "<font size=4 color=blue>**正向传播计算图**</font>\n",
    "\n",
    "方框代表变量，圆圈代表运算符，箭头表示从输入到输出的依赖关系。\n",
    "\n",
    "<img src = \"forward_pro.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>**2、反向传播**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向传播（back-propagation)** 指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数梯度。\n",
    "\n",
    "对输入或输出 $X,Y,Z$ 为任意形状张量的函数 $Y = f(X)$ 和 $Z = g(Y)$，通过链式法则，有<font size=4> $\\frac{\\partial Z}{\\partial X} = prob\\big( \\frac{\\partial Z}{\\partial Y} , \\frac{\\partial Y}{\\partial X}  \\big)$</font>，其中 $prob$ 运算符根据两个输入的形状，在必要的操作（如转置和互换输入位置）后对两个输入做乘法。\n",
    "\n",
    "本节样例模型中，参数为 $W^{(1)},W^{(2)}$，因此反向传播的目标是计算 <font size=4>$\\frac{\\partial J}{\\partial W^{(1)}}$</font> 和<font size=4> $\\frac{\\partial J}{\\partial W^{(2)}}$ </font> \n",
    "\n",
    "我们将应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序正好相反。\n",
    "\n",
    "首先，分别计算目标函数 $J = L + s$ 有关损失项 $L$ 和正则项 $s$ 的梯度： <font size=4> $\\frac{\\partial J}{\\partial L} = 1, \\frac{\\partial J}{\\partial s} = 1$ </font>\n",
    "\n",
    "其次，依据链式法则计算目标函数有关输出层变量的梯度 <font size=4>$\\frac{\\partial J}{\\partial o}\\in \\mathcal{R}^q$ ： $\\frac{\\partial J}{\\partial o} = prod \\big(\\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial o}\\big) = \\frac{\\partial J}{\\partial o}$</font>\n",
    "\n",
    "接下来计算正则项有关两个参数的梯度：<font size=4> $ \\frac{\\partial s}{\\partial W^{(1)}} = \\lambda W^{(1)}, \\frac{\\partial s}{\\partial W^{(2)}} = \\lambda W^{(2)} $  \n",
    "    \n",
    "现在，计算最靠近输出层模型参数的梯度 <font size=4> $\\frac{\\partial J}{\\partial W^{(2)}} \\in \\mathcal{R}^{q\\times h}$ ：$\\frac{\\partial J}{\\partial W^{(2)}} = prod\\big( \\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial W^{(2)}} \\big) + prod\\big( \\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(2)}} \\big) = \\frac{\\partial J}{\\partial o}h^T + \\lambda W^{(2)}$</font>  \n",
    "    \n",
    "沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度 <font size=4>$\\frac{\\partial J}{\\partial h}\\in \\mathcal{R}^h$： $\\frac{\\partial J}{\\partial h} = prod\\big( \\frac{\\partial J}{\\partial o}, \\frac{\\partial o}{\\partial h}\\big) = {W^{(2)}}^T \\frac{\\partial J}{\\partial o}$</font> \n",
    "    \n",
    "由于激活函数 $\\phi$ 是按元素运算的，中间变量 $z$ 的梯度 $\\frac{\\partial J}{\\partial z}\\in \\mathcal{R}^h$ 的计算需要使用按元素乘法符 $\\Theta$： <font size=4> $\\frac{\\partial J}{\\partial z} = prod \\big( \\frac{\\partial J}{\\partial h}, \\frac{\\partial h}{\\partial z} \\big) = \\frac{\\partial J}{\\partial h}\\Theta \\phi^{'}(z)$ </font> \n",
    "    \n",
    "最终，可以得到最靠近输入层的模型参数的梯度 <font size=4> $\\frac{\\partial J}{\\partial W^{(1)}}\\in \\mathcal{R}^{h\\times d}$： $\\frac{\\partial J}{\\partial W^{(1)}} = prod\\big( \\frac{\\partial J}{\\partial z}, \\frac{\\partial z}{\\partial W^{(1)}}\\big) + prod\\big( \\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial W^{(1)}}\\big)= \\frac{\\partial J}{\\partial z}x^T + \\lambda W^{(1)}$ </font> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
