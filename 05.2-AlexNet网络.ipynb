{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AlexNet网络 - 深度卷积神经网络**\n",
    "\n",
    "\n",
    "**参考文献：** [1] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AlexNet.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end)的方法节省了很多中间步骤。然而，在很长一段时间里更流行的是研究者通过勤劳与智慧所设计并生成的手工特征。\n",
    "这类图像分类研究的主要流程是：1）获取图像数据集；2）使用已有的特征提取函数生成图像的特征；3）使用机器学习模型对图像的特征分类。\n",
    "当时认为的机器学习部分仅限最后这一步。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2012 AlexNet 使用了8层卷积神经网络，它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的现状。\n",
    "\n",
    "AlexNet 与 LeNet 的设计理论非常相似，但是也有显著的区别。\n",
    "\n",
    "**第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。**\n",
    "\n",
    " Alex 第一层中的卷积窗口形状是 $11\\times 11$。 因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。\n",
    " \n",
    " 第二层中的卷积窗口形状减小到 $5\\times 5$，之后全采用 $3\\times 3$。此外，第一、第二和第五个卷积层之后都使用了窗口形状为$3\\times 3$、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。\n",
    " \n",
    " 紧接着，最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1GB 的模型参数。由于早期显存的限制，最早的AlexNet 使用双数据流的设计使一块GPU只需要处理一半模型。\n",
    " \n",
    "**第二，AlexNet 将sigmoid激活函数改成了更加简单的ReLU激活函数。** \n",
    " \n",
    " 一方面，ReLU激活函数的计算更简单，例如它没有sigmoid激活函数中的求幂运算。\n",
    " \n",
    " 另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间内得到几乎为0的梯度，从而令模型无法得到有效的训练。\n",
    " \n",
    "**第三，AlexNet通过丢弃法，来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法**\n",
    " \n",
    "**第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet 与LeNet结构类似，但是使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。**\n",
    "\n",
    "AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序做了大量的调整。虽然，AlexNet指明了深度卷积神经网络可以取得出色的结果，但是并没有提供简单的规则以指导后来的研究者如何设计新的网络。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.gluon import data as gdata,nn\n",
    "import os\n",
    "import sys \n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "# 使用较大的 11*11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。 这里使用输出通道数也比LeNet大很多\n",
    "\n",
    "net.add(nn.Conv2D(96,kernel_size = 11, strides = 4, activation=\"relu\"),\n",
    "        nn.MaxPool2D(pool_size = 3, strides = 2),\n",
    "        \n",
    "        # 减小卷积窗口，使用填充为2使得输入与输出的高与宽一致，且增大输出通道\n",
    "        nn.Conv2D(256,kernel_size = 5, padding = 2, activation = 'relu'),\n",
    "        nn.MaxPool2D(pool_size = 3, strides = 2),\n",
    "        \n",
    "        # 前2个卷积层后不使用池化层来减小输入的高和宽\n",
    "        nn.Conv2D(384, kernel_size = 3, padding = 1, activation = 'relu'),\n",
    "        nn.Conv2D(384, kernel_size = 3, padding = 1, activation = 'relu'),\n",
    "        nn.Conv2D(256, kernel_size = 3, padding = 1, activation = 'relu'),\n",
    "        nn.MaxPool2D(pool_size = 3, strides = 2),\n",
    "        \n",
    "        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        nn.Dense(4096, activation = 'relu'),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Dense(4096, activation = 'relu'), \n",
    "        nn.Dropout(0.5),\n",
    "        \n",
    "        # 输出层 由于这里使用Fashion_MNIST，所以类别为10\n",
    "        nn.Dense(10)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.conv2D:\n",
    "#     out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1\n",
    "#     out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1\n",
    "    \n",
    "# nn.MaxPool2D:\n",
    "#     out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1\n",
    "#     out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 \t Input [nh,hw] = [224 , 224] ; Kernel [kh,kw] = [11 , 11] ; Padding [ph,pw] = [0 , 0] ; Strid: 4 \n",
      "\t OutputShape [54 , 54]\n",
      "Pool0 \t Input [nh,hw] = [54 , 54] ; PoolSize : [3 , 3] ; Padding [ph,pw] = [0 , 0] ; Strid: 2 \n",
      "\t OutputShape [26 , 26]\n",
      "conv1 \t Input [nh,hw] = [26 , 26] ; Kernel [kh,kw] = [5 , 5] ; Padding [ph,pw] = [2 , 2] ; Strid: 1 \n",
      "\t OutputShape [26 , 26]\n",
      "Pool1 \t Input [nh,hw] = [26 , 26] ; PoolSize : [3 , 3] ; Padding [ph,pw] = [0 , 0] ; Strid: 2 \n",
      "\t OutputShape [12 , 12]\n",
      "conv2 \t Input [nh,hw] = [12 , 12] ; Kernel [kh,kw] = [3 , 3] ; Padding [ph,pw] = [1 , 1] ; Strid: 1 \n",
      "\t OutputShape [12 , 12]\n",
      "conv3 \t Input [nh,hw] = [12 , 12] ; Kernel [kh,kw] = [3 , 3] ; Padding [ph,pw] = [1 , 1] ; Strid: 1 \n",
      "\t OutputShape [12 , 12]\n",
      "conv4 \t Input [nh,hw] = [12 , 12] ; Kernel [kh,kw] = [3 , 3] ; Padding [ph,pw] = [1 , 1] ; Strid: 1 \n",
      "\t OutputShape [12 , 12]\n",
      "Pool2 \t Input [nh,hw] = [12 , 12] ; PoolSize : [3 , 3] ; Padding [ph,pw] = [0 , 0] ; Strid: 2 \n",
      "\t OutputShape [5 , 5]\n"
     ]
    }
   ],
   "source": [
    "#######################################################卷积层&池化 输出尺寸计算\n",
    "import numpy as np \n",
    "\n",
    "def cal_conv_output_size(n,kernel_size,padding_size,stride):\n",
    "    if (n + 2*padding_size - (kernel_size - 1))%stride==0:\n",
    "        return (n + 2*padding_size - (kernel_size - 1))/ stride\n",
    "    else:\n",
    "        return np.floor((n + 2*padding_size - (kernel_size - 1))/stride) + 1\n",
    "\n",
    "def cal_pooling_output_size(n,pool_size,padding_size,stride):\n",
    "    if (n + 2*padding_size - pool_size) % stride == 0 :\n",
    "        return (n + 2*padding_size - pool_size) / stride\n",
    "    else:\n",
    "        return np.floor((n + 2*padding_size - pool_size) / stride) + 1\n",
    "    \n",
    "\n",
    "nh,nw,kh,kw,ph,pw,sh,sw = 224,224,11,11,0,0,4,4\n",
    "print(\"conv0 \\t Input [nh,hw] = [%d , %d] ; Kernel [kh,kw] = [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,kh,kw,ph,pw,sh ))\n",
    "nh, nw = cal_conv_output_size(nh, kh,ph,sh), cal_conv_output_size(nw,kw,pw,sw)\n",
    "# nh, nw = np.floor((nh + 2*ph - (kh - 1))/sh) + 1, np.floor((nw + 2*pw - (kw - 1))/sw) + 1\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "# kh,kw = 3,3 # pooling\n",
    "sh,sw = 2,2 # stride\n",
    "pool_size = 3\n",
    "\n",
    "print(\"Pool0 \\t Input [nh,hw] = [%d , %d] ; PoolSize : [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,pool_size,pool_size,ph,pw,sh ))\n",
    "nh, nw = cal_pooling_output_size(nh, pool_size, ph,sh), cal_pooling_output_size(nw, pool_size, pw, sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "kh,kw = 5,5 # kernel \n",
    "ph,pw = 2,2\n",
    "sh,sw = 1,1 # 上面 Conv1中没有定义stride,默认值为1\n",
    "\n",
    "print(\"conv1 \\t Input [nh,hw] = [%d , %d] ; Kernel [kh,kw] = [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,kh,kw,ph,pw,sh ))\n",
    "nh, nw = cal_conv_output_size(nh, kh,ph,sh), cal_conv_output_size(nw,kw,pw,sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "sh,sw = 2,2\n",
    "pool_size = 3 \n",
    "ph,pw = 0,0\n",
    "print(\"Pool1 \\t Input [nh,hw] = [%d , %d] ; PoolSize : [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,pool_size,pool_size,ph,pw,sh ))\n",
    "nh, nw = cal_pooling_output_size(nh, pool_size, ph,sh), cal_pooling_output_size(nw, pool_size, pw, sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "kh,kw = 3,3\n",
    "ph,pw = 1,1\n",
    "sh,sw = 1,1\n",
    "print(\"conv2 \\t Input [nh,hw] = [%d , %d] ; Kernel [kh,kw] = [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,kh,kw,ph,pw,sh ))\n",
    "nh, nw = cal_conv_output_size(nh, kh,ph,sh), cal_conv_output_size(nw,kw,pw,sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "print(\"conv3 \\t Input [nh,hw] = [%d , %d] ; Kernel [kh,kw] = [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,kh,kw,ph,pw,sh ))\n",
    "nh, nw = cal_conv_output_size(nh, kh,ph,sh), cal_conv_output_size(nw,kw,pw,sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "print(\"conv4 \\t Input [nh,hw] = [%d , %d] ; Kernel [kh,kw] = [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,kh,kw,ph,pw,sh ))\n",
    "nh, nw = cal_conv_output_size(nh, kh,ph,sh), cal_conv_output_size(nw,kw,pw,sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n",
    "\n",
    "\n",
    "sh,sw = 2,2\n",
    "pool_size = 3 \n",
    "ph,pw = 0,0\n",
    "print(\"Pool2 \\t Input [nh,hw] = [%d , %d] ; PoolSize : [%d , %d] ; Padding [ph,pw] = [%d , %d] ; Strid: %d \"%( nh,nw,pool_size,pool_size,ph,pw,sh ))\n",
    "nh, nw = cal_pooling_output_size(nh, pool_size, ph,sh), cal_pooling_output_size(nw, pool_size, pw, sw)\n",
    "print(\"\\t OutputShape [%d , %d]\" %(nh,nw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test \n",
    "\n",
    "# temp_X = nd.random.uniform(shape = (1,1,224,224))\n",
    "# net.initialize()\n",
    "\n",
    "# for layer in net:\n",
    "#     temp_X = layer(temp_X)\n",
    "#     print(layer.name, \"output shape:\\t\",temp_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize = None, root = os.path.join(\"~\",\".mxnet\",'datasets','fashion-mnist')):\n",
    "\n",
    "    root = os.path.expanduser(root) # 展开用户路径 ~\n",
    "    transformer = []\n",
    "    \n",
    "    # 读取数据的时候，将图像高和宽扩大到AlexNet中使用的图像大小，即224，这里是通过Resize实现的；\n",
    "    # 也就是说，在ToTensor实例前使用Resize实例，然后通过Compose实例来将这两个变换串联以方便调用\n",
    "    \n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "        \n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "    \n",
    "    mnist_train = gdata.vision.FashionMNIST(root = root,train = True)\n",
    "    mnist_test = gdata.vision.FashionMNIST(root = root, train = False)\n",
    "    \n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4 \n",
    "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle = True, num_workers = num_workers)\n",
    "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, shuffle = False, num_workers = num_workers)\n",
    "    \n",
    "    return train_iter,test_iter\n",
    " \n",
    "batch_size = 128\n",
    "\n",
    "# 如果出来 \"out of memory\" 的报错信息，可减小 batch_size 或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.3100, train acc 0.509, test acc 0.749, time 109.8 sec\n",
      "epoch 2, loss 0.6509, train acc 0.756, test acc 0.801, time 107.0 sec\n",
      "epoch 3, loss 0.5350, train acc 0.800, test acc 0.833, time 104.3 sec\n",
      "epoch 4, loss 0.4681, train acc 0.828, test acc 0.856, time 103.5 sec\n",
      "epoch 5, loss 0.4261, train acc 0.845, test acc 0.866, time 100.6 sec\n",
      "epoch 6, loss 0.3952, train acc 0.856, test acc 0.874, time 100.4 sec\n",
      "epoch 7, loss 0.3742, train acc 0.865, test acc 0.879, time 100.4 sec\n",
      "epoch 8, loss 0.3549, train acc 0.871, test acc 0.883, time 100.5 sec\n",
      "epoch 9, loss 0.3414, train acc 0.875, test acc 0.889, time 100.9 sec\n",
      "epoch 10, loss 0.3276, train acc 0.881, test acc 0.892, time 102.8 sec\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "lr, num_epochs, ctx = 0.01, 10, d2l.try_gpu()\n",
    "net.initialize(force_reinit = True, ctx = ctx, init = init.Xavier())\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch5( net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
