{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet - 残差网络**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **残差块**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们希望学出的理想映射为$f(x)$，作为图5.9上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射 $f(x)$，而右图虚线框中的部分则需要拟合出有关司徒映射的残差映射$f(x) - x$ . 残差映射在实际中往往更容易优化。\n",
    "\n",
    "只需将右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么 $f(x)$ 即为恒等映射。而实际中，当理想映射 $f(x)$ 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。\n",
    "\n",
    "图5.9右图也是ResNet的基础块，即残差块（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ResNet.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet没用了VGG 全$3\\times 3$卷积层的设计。残差块里首先有2个相同输出通道数的 $3\\times 3$卷积层。 每个卷积层后接一个批量归一化层和ReLU激活函数。然后，将输入跳过这2个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求2个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 $1\\times 1$卷积层来将输入变换成需要的形状后再做相加运算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon,init,nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Residual(nn.Block):\n",
    "    def __init__(self,num_channels,use_1X1conv = False,strides = 1,**kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size = 3, padding = 1, strides = strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size = 3, padding = 1)\n",
    "        if use_1X1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size = 1, strides = strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return nd.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 3, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看输入和输出形状\n",
    "X = nd.random.uniform(shape=(4, 3, 6, 6))\n",
    "blk = Residual(6, use_1X1conv=True, strides=2)\n",
    "blk.initialize()\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ResNet模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet的前两层跟之前介绍的GoogleNet中的一样：在输出通道为64、步幅为2的 $7\\times 7$卷积层后接步幅为2的$3\\times 3$ 的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(64, kernel_size = 7, strides = 2, padding = 3),\n",
    "        nn.BatchNorm(), nn.Activation(\"relu\"),\n",
    "        nn.MaxPool2D(pool_size = 3, strides = 2, padding = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet在后面接一4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。\n",
    "\n",
    "第一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(num_channels, num_residuals, first_block = False):\n",
    "    blk = nn.Sequential()\n",
    "    for i in range(num_channels):\n",
    "        if i ==0 and not first_block:\n",
    "            blk.add(Residual(num_channels,use_1X1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为ResNet加入所有残差块。这里每个模块使用2个残差块。\n",
    "\n",
    "net.add(resnet_block(64, 2, first_block=True),\n",
    "       resnet_block(128,2),\n",
    "      resnet_block(256,2),\n",
    "      resnet_block(512,2))\n",
    "\n",
    "# 加入全局平均池化层后接上全连接层输出\n",
    "net.add(nn.GlobalAvgPool2D(), nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，每个模块里有4个卷积（不计算1X1卷积层），加上最开始的卷积层和最后的全连接层，共18层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据\n",
    "lr, num_epochs, batch_size, ctx = 0.05, 5, 256, d2l.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
